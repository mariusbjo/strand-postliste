# Strand kommune postliste-scraper

Dette prosjektet automatiserer innhenting og publisering av kommunens postliste (journalposter) ved hjelp av Python-scrapere og GitHub Actions.  
Systemet hÃ¥ndterer bÃ¥de daglige oppdateringer og full historisk scraping.

## ğŸš€ Funksjoner

- **Daglig scraping (incremental)**  
  - Workflow: `morgen.yml`  
  - KjÃ¸rer hver morgen kl. 06:00.  
  - Henter nye oppfÃ¸ringer og oppdaterer eksisterende.  
  - Bruker `scraper.py` i *incremental*-modus.

- **Oppdaterings-scraping (update)**  
  - Workflow: `oppdatering.yml`  
  - Kan kjÃ¸res manuelt eller planlagt.  
  - GÃ¥r gjennom de siste 50 sidene (konfigurerbart).  
  - Henter bÃ¥de nye oppfÃ¸ringer og oppdaterer eksisterende.

- **Publisering (HTML)**  
  - Workflow: `publish.yml`  
  - Genererer `index.html` fra `postliste.json`.  
  - Publiserer oppdatert postliste som statisk HTML.

- **Full historisk scraping**  
  - Workflow: `fullscrape.yml`  
  - Brukes til Ã¥ hente hele Ã¥r eller halvÃ¥r.  
  - Benytter `scraper_dates.py` med dato-intervall.  
  - Resultat lagres i `archive/` som egne JSON-filer (f.eks. `postliste_2006_H1.json`).

## âš™ï¸ Konfigurasjon

Alle scrapere leser innstillinger fra `config.json`.  
Eksempel pÃ¥ innhold:

```json
{
  "mode": "incremental",
  "max_pages_incremental": 10,
  "max_pages_update": 50,
  "max_pages_full": 200,
  "per_page": 100
}

mode: styrer hvordan scraperen kjÃ¸rer (incremental, update, full).
max_pages_incremental: antall sider som sjekkes i daglig scraping.
max_pages_update: antall sider som sjekkes i oppdateringsmodus.
max_pages_full: antall sider som sjekkes i full scraping.
per_page: antall oppfÃ¸ringer per side (100 anbefales).
For fullscrape.yml brukes en egen config_fullscrape.json slik at config.json for daglig drift ikke overskrives.

---

### 4. Scrapere
```markdown
## ğŸ Scrapere

### `scraper.py`
- Brukes av `morgen.yml`.  
- KjÃ¸rer i incremental-modus.  
- Stopper fÃ¸rst nÃ¥r alle oppfÃ¸ringer pÃ¥ en side er kjente.  
- Henter bÃ¥de nye og oppdaterte oppfÃ¸ringer.

### `scraper_dates.py`
- Brukes av `fullscrape.yml`.  
- KjÃ¸rer i full-modus.  
- Tar inn dato eller periode som argument:
  ```bash
  python scraper_dates.py 2025-12-01
  python scraper_dates.py 2025-01-01 2025-12-31

## ğŸ“‚ Filstruktur
â”œâ”€â”€ archive/ # Historiske JSON-filer (fullscrape)
â”œâ”€â”€ postliste.json # Hovedfil med siste oppfÃ¸ringer
â”œâ”€â”€ index.html # Generert HTML fra postliste.json
â”œâ”€â”€ scraper.py # Incremental scraper
â”œâ”€â”€ scraper_dates.py # Full scraper med dato-intervall
â”œâ”€â”€ generate_html.py # Lager HTML fra JSON
â”œâ”€â”€ config.json # Daglig konfigurasjon
â”œâ”€â”€ config_fullscrape.json # Fullscrape-konfigurasjon
â””â”€â”€ .github/workflows/ # GitHub Actions workflows

## ğŸ”„ Workflows

- **morgen.yml** â†’ daglig incremental scraping + HTML.  
- **oppdatering.yml** â†’ manuell eller planlagt update-scraping.  
- **publish.yml** â†’ genererer og publiserer HTML.  
- **fullscrape.yml** â†’ full historisk scraping (halvÃ¥r/Ã¥r).

## ğŸ“Š Output

- JSON-filer (`postliste.json` og arkivfiler) med alle oppfÃ¸ringer.  
- HTML (`index.html`) som viser postlisten i lesbart format.  
- OppfÃ¸ringer inneholder:
  - `tittel`
  - `dato` (dd.mm.yyyy)
  - `parsed_date` (ISO)
  - `dokumentID`
  - `dokumenttype`
  - `avsender_mottaker`
  - `journal_link`
  - `filer`
  - `status`

## ğŸ› ï¸ Bruk

- Daglig drift skjer automatisk via GitHub Actions.  
- Fullscrape trigges manuelt via `workflow_dispatch`.  
- Alle endringer commitâ€‘tes og pushes automatisk til repoet.

## ğŸ“Œ Viktig

- **Incremental-modus** stopper fÃ¸rst nÃ¥r alle oppfÃ¸ringer pÃ¥ en side er kjente.  
- **Update-modus** henter bÃ¥de nye og oppdaterte oppfÃ¸ringer.  
- **Full-modus** brukes for historiske perioder og henter opptil 200 sider.  
- `config.json` er den eneste kilden til sannhet for scraperâ€‘innstillinger.
